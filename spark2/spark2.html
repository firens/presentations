<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Spark 2</title>

    <meta name="description" content="Spark 2, loads of new features">
    <meta name="author" content="Samuel Durand">

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="../revealjs/css/reveal.css">
    <link rel="stylesheet" href="../revealjs/css/theme/black.css" id="theme">
    <link rel="stylesheet" href="css/spark2.css"/>

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="../revealjs/lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? '../revealjs/css/print/pdf.css' : '../revealjs/css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">

    <div class="slides">
        <section>
            <h1 style="text-transform: none">Spark 2.0</h1>
            <img class="logo" src="img/spark-logo.png"/>
            <p style="margin-top: 1em">
                <small>Presentation by <a href="https://twitter.com/samudurand">Samuel Durand</a></small>
            </p>
        </section>

        <section>
            <h3 style="margin-bottom: 1em">Today we will talk about...</h3>
            <ul>
                <li>What is Spark ?</li>
                <li>What's new in Spark 2.0</li>
                <li>Most interesting features</li>
                <li>Future</li>
                <li>Demo</li>
            </ul>
        </section>

        <section>

            <section>
                <h2 style="margin-bottom: 1em">Apache Spark</h2>
                <p>Created in <b>2009</b> by  Matei Zaharia</p>
                <p>Maintained since <b>2013</b> by the Apache Foundation</p>
                <br/>
                <p>Version <b class="highlight-orange">2.0</b> released in June <b>2016</b></li></p>
                <p>One of the most active Apache project</p>
            </section>

            <section>
                <h2 style="margin-bottom: 1em">What is Spark ?</h2>
                <p style="margin-bottom: 1em">Advertised as <i>"A fast and general engine for large-scale data processing"</i></p>
                <p style="margin-bottom: 1em">A <span class="highlight-orange">data processing</span> platform,
                    running on a <span class="highlight-orange">cluster</span></p>
                <p>Designed to handle very large amount of archived or live data very <span class="highlight-orange">quickly</span> and <span class="highlight-orange">efficiently</span>.</p>
            </section>

            <section>
                <h2 style="margin-bottom: 1em">What is Spark ?</h2>
                <ul>
                    <li>Designed for <span class="highlight-orange">analytical</span> and transformation operations</li>
                    <li>Little amount of code required to use it</li>
                    <li>Available in Scala, Java, Python and R</li>
                    <li>Handle data <span class="highlight-orange">in memory</span> for speed</li>
                    <li>Can work on very large cluster (> 8000 nodes)</li>
                </ul>
            </section>

            <section>
                <h2 style="margin-bottom: 1em">Features</h2>
                <ul>
                    <li style="margin-bottom: 1em">Data analysis of large amount of data</li>
                    <li style="margin-bottom: 1em">Manipulation of RDDs by Scala like code <br/>or SQL queries</li>
                    <li style="margin-bottom: 1em">Live data streaming</li>
                    <li style="margin-bottom: 1em">Machine Learning</li>
                    <li>APIs for many datasources <br/>(HDFS, Cassandra, Kafka...)</li>
                </ul>
            </section>

            <section>
                <h2>Spark Ecosystem</h2>
                <img class="logo" src="img/spark-ecosystem.jpg"/>
            </section>

            <section>
                <h2>What it looks like</h2>
                <pre><code data-trim  class="scala">
val rdd = sparkContext.textFile("hdfs://...")

rdd.flatMap(line => line.split(" "))
   .map(word => (word, 1))
   .reduceByKey(_ + _)
   .saveAsTextFile("hdfs://...")
                </code></pre>
                <pre><code data-trim  class="scala">
val data = Array("Spark has potential",
                 "Storm is not bad either",
                 "Scala is the best for Spark !")
val rdd = data.parallelize(data)
val strWithSpark = rdd.filter(line => line.contains("Spark"))

// Cache the RDD in memory for later use
strWithSpark.cache

strWithSpark.forEach(println)
                </code></pre>
            </section>

            <section>
                <h2 style="margin-bottom: 1em">A note on Databricks</h2>
                <p>Founded by Matei Zaharia and other co-founders of Spark</p>
                <img class="logo" src="img/databricks-logo.png"/>
                <p>Provides services and support for Spark</p>
                <p>Also offers an <span class="highlight-orange">online platform</span> for experimenting/training (free option since June 2016)</p>
            </section>

        </section>

        <section>
            <h2>Spark 2 : What's new ?</h2>
            <ul>
                <li class="fragment">Clear focus on <i>Datasets/Dataframes</i> instead of <i>RDDs</i></li>
                <li class="fragment">Structured Streaming</li>
                <li class="fragment"><b class="highlight-orange">SparkSession</b> : single entry point to data APIs</li>
                <li class="fragment"><b class="highlight-orange">SparkSQL</b> : all 99 TPC-DS queries</li>
                <li class="fragment">Performance improvements : <b class="highlight-orange">Tungsten</b></li>
                <li class="fragment">Spark.ML replaces MLlib</li>
                <li class="fragment">Event time window functions on streaming</li>
                <li class="fragment">Many more : CSV support, more features for R users...</li>
            </ul>
        </section>

        <section>

            <section>

                <h2>RDDs VS Datasets</h2>

                <p>Better performances for less efforts</p>

            </section>

            <section>

                <h2 style="margin-bottom: 1em">Resilient Distributed Dataset (RDD)</h2>

                <ul>
                    <li class="fragment">Immutable distributed collection of Objects</li>
                    <div class="fragment">
                        <li>Unstructured data</li>
                        <pre><code data-trim  class="scala">
val lines = sc.textFile("data.txt")
val total = lines.map(s => s.length)
                 .reduce((a, b) => a + b)
                        </code></pre>
                    </div>
                    <li class="fragment">Functional programming</li>
                    <li class="fragment">Syntax errors caught at compile time</li>
                    <li class="fragment">Limited optimizations</li>
                    <li class="fragment">More control over operations</li>
                </ul>

            </section>

            <section>

                <h2 style="margin-bottom: 1em">Dataframes</h2>

                <ul>
                    <li class="fragment">A distributed collection of data organized into named columns</li>
                    <li class="fragment">Equivalent to a <b class="highlight-orange">table</b> in a relational database</li>
                    <li class="fragment">Handles <b class="highlight-orange">structured data</b>, can infer the schema</li>
                    <li class="fragment">Higher abstraction on top of RDDs, <b class="highlight-orange">less control</b></li>
                    <li class="fragment">Designed to simplify the tasks of users</li>
                    <li class="fragment">Optimization and code generation : Spark SQL <b class="highlight-orange">Catalyst</b></li>
                    <li class="fragment">In Spark 2, only a subtype of Datasets</li>
                </ul>

            </section>

            <section>

                <h2 style="margin-bottom: 1em">Datasets</h2>

                <ul>
                    <li class="fragment">Immutable distributed collection of Objects</li>
                    <li class="fragment">Abstraction on top of Dataframe : <b class="highlight-orange">structured data</b></li>
                    <li class="fragment">3 APIs available : SQL, Dataframe, Dataset</li>
                    <li class="fragment"><b class="highlight-orange">Type safe : operate on domain objects with DSL or lambda functions</b></li>
                    <li class="fragment">Syntax and analysis errors caught at <b class="highlight-orange">compile time</b></li>
                    <li class="fragment">Structure <b class="highlight-orange">limits</b> what can be done :
                        <br/><b class="highlight-orange">Storage and performance optimizations</b></li>
                </ul>

            </section>

            <section>

                <h2>Structured APIs</h2>

                <p>SQL</p>
                <pre><code data-trim  class="scala">
val peopleDF = spark.read.json("people.json")
peopleDF.createOrReplaceTempView("people")
spark.sql("SELECT name FROM people").show()
                </code></pre>
                <p>Dataframes = Dataset[Row]</p>
                <pre><code data-trim  class="scala">
val peopleDF = spark.read.json("people.json")
peopleDF.select($"name", ($"age" < 25).as("young"))
        .orderBy($"young".desc, $"name").show()
                </code></pre>
                <p>Datasets</p>
                <pre><code data-trim  class="scala">
val personDS = spark.read.json("people.json").as[Person]
personDs.foreach(p => println(s"| ${p.name} | ${p.age + 1} |"))
                </code></pre>
            </section>
            
            <section>
                
                <h2>Structured APIs</h2>
                
                <img src="img/structured-apis.png">

                <div class="fragment">
                    <p >Analysis errors caught before job starts</p>

                    <pre><code data-trim  class="scala">
case class University(numStudents: Int)
val schools = sqlContext.read.json("/schools.json").as[University]
org.....AnalysisException:
  Cannot upcast `numStudents` from string to int as it may truncate
                    </code></pre>
                </div>
                
            </section>

            <section>

                <h2>Better performances ?</h2>

            </section>

            <section>

                <h2>Catalyst : Query planner</h2>

                <p>Decides the best way to perform a query by trying several possibilities</p>

                <img src="img/catalyst.png"/>

            </section>

            <section>

                <h2>Catalyst : Query planner</h2>

                <img src="img/catalyst-optimization.png"/>

            </section>

            <section>

                <h2>Optimizations</h2>

                <p>Better <b class="highlight-orange">performances</b> with relational execution engine</p>
                <img src="img/performance.png"/>

                <p>Spark knows structure of data : it can optimize <b class="highlight-orange">storage</b></p>
                <img src="img/memory-usage.png"/>

            </section>

            <section>

                <h2>Optimizations</h2>

                <p>Same performances for all languages</p>
                <img src="img/same-perfs.png"/>

            </section>

            <section>

                <h2>Optimizations</h2>

                <p>Serialized using <b class="highlight-orange">Encoders</b> into Tungsten binary format</p>

                <p>Far better performances with less space (up to 2x less)</p>

                <img src="img/encoder-performances.png"/>

                <p>Operate Directly On Serialized Data when possible</p>

            </section>

            <section>

                <h2>Optimizations</h2>

                <p>Spark works only with a limited number of types<br/>
                   => can use its own compact encoding
                </p>

                <img class="logo" src="img/tungsten-format.png">

                <p>Encoders translate between domain objects and Spark's internal format</p>

            </section>

        </section>

        <section>

            <section>

                <h2>SparkSession</h2>

                <p>A unique entry point to all APIs</p>

            </section>

            <section>

                <h2 style="margin-bottom: 1em">Before</h2>

                <p>SparkContext : RDD Api</p>
                <pre><code data-trim  class="scala">
val conf = new SparkConf().setAppName("Spark App")
                      .setMaster("local[2]")
val sc = new SparkContext(conf)
                </code></pre>

                <p>SqlContext : Dataframe Api</p>
                <pre><code data-trim  class="scala">
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
                </code></pre>

                <p><div style="display: inline-block">HiveContext : Dataframe Api, Window functions <br/>
                        and Hive compatibility</div> <img class="logo" src="img/hive-logo.png"/> </p>
                <pre><code data-trim  class="scala">
val hContext = new org.apache.spark.sql.hive.HiveContext(sc)
                </code></pre>


            </section>

            <section>

                <h2>Problems</h2>

                <p style="margin-bottom: 1em"><b class="highlight-orange">Confusion</b> between SqlContext and HiveContext : which one to choose ?</p>

                <p style="margin-bottom: 1em">Heavy dependence on Hive especially for <b class="highlight-orange">window functions</b></p>

                <p>Hive has lots of <b class="highlight-orange">dependencies</b></p>

            </section>

            <section>

                <h2>In Spark 2</h2>
                <pre><code data-trim  class="scala">

val spark = SparkSession.builder.master("local[2]")
                                .appName("Spark app")
                                .getOrCreate()

val personsDF = spark.read.json("persons.json")

val personsDS = personsDF.as[Person]
                </code></pre>

                <div class="fragment">
                    <p style="margin-top: 1em; margin-bottom: 1em">The SparkContext is also available</p>

                    <pre><code data-trim  class="scala">
val sc = spark.sparkContext
                    </code></pre>

                 </div>

                <p class="fragment">But... still dependent on Hive</p>
            </section>

        </section>

        <section>

            <section>

                <h2>Tungsten 2nd generation</h2>

            </section>

            <section>

                <h2>What is Tungsten ?</h2>

                <p>Project aiming to improve the efficiency of memory and CPU for Spark applications.</p>

                <ul class="fragment">
                    <li style="margin-bottom: 2%;">Memory : eliminate the <b class="highlight-orange">overhead</b> of JVM object model</li>
                    <li style="margin-bottom: 2%;">Find ways around the limitations of <b class="highlight-orange">garbage collection</b></li>
                    <li style="margin-bottom: 2%;"><b class="highlight-orange">Cache-aware</b> computation: exploit memory hierarchy</li>
                    <li style="margin-bottom: 2%;">Improve <b class="highlight-orange">bytecode generation</b> to exploit modern compilers and CPUs</li>
                </ul>

            </section>

            <section>

                <h2>Tungsten 2nd Generation</h2>

                <p>Introduce a new mechanism : <i>whole-stage code generation</i></p>

                <p class="fragment">Emits <b class="highlight-orange">optimized bytecode</b> at runtime that collapses
                    the entire query into a <b class="highlight-orange">single function</b></p>

                <ul>
                    <li class="fragment">Eliminates virtual function calls</li>
                    <li class="fragment">Uses CPU registers for intermediate data</li>
                </ul>

            </section>

            <section>

                <h2 style="margin-bottom: 1em">Volcano Iterator Model</h2>

                <img class="fragment logo" data-fragment-index="1" style="margin-left:1em; float: right;" src="img/volcano-iterator-model.png"/>

                <p style="margin-bottom: 2em">A query consists of multiple operators, and each operator presents an interface, next()</p>

                <p class="fragment" style="margin-bottom: 2em" data-fragment-index="1"><i>select count(*) from store_sales<br/> where ss_item_sk = 1000</i></p>

                <ul class="fragment" data-fragment-index="2" style="width: 65%">
                    <li>Process any combination of operations</li>
                    <li>No concerns on the type of data</li>
                </ul>

            </section>

            <section>

                <h2 style="margin-bottom: 1em">Compared with hand-written code ?</h2>

                <pre style="box-shadow: none"><code data-trim  class="java">
var count = 0
for (ss_item_sk in store_sales) {
  if (ss_item_sk == 1000) {
    count += 1
  }
}
                </code></pre>

                <img style="margin-top: 1em" class="fragment" src="img/volcano-vs-hand-written-code.png"/>

            </section>

            <section>

                <h2>Why such a difference ?</h2>

                <ul>
                    <li>No virtual function calls</li>
                    <li>No need to save data in memory</li>
                    <li>CPUs are incredibly efficient when handling for loops</li>
                </ul>

            </section>

            <section>

                <h2>Whole-stage code generation</h2>

                <img style="background: white; padding: 15px" class="logo" src="img/whole-stage-code-generation-model.png"/>

            </section>

            <section>

                <h2 style="margin-bottom: 1em">Performances</h2>

                <table>
                    <thead>
                        <tr><td>Primitive</td><td>Spark 1.6</td><td>Spark 2.0</td></tr>
                    </thead>
                    <tbody>
                        <tr><td>filter</td><td>15 ns</td><td>1.1 ns</td></tr>
                        <tr><td>sum</td><td>14 ns</td><td>0.9 ns</td></tr>
                        <tr><td>hash join</td><td>115 ns</td><td>4.0 ns</td></tr>
                        <tr><td>sort</td><td>620 ns</td><td>5.3 ns</td></tr>
                    </tbody>
                </table>

            </section>

        </section>

        <section>

            <section>

                <h2>Structured Streaming</h2>

                <p>The simplest way to perform streaming analytics
                    is not having to reason about streaming.</p>

            </section>

            <section>

                <h2 style="margin-bottom: 1em">Structured Streaming in Spark 2</h2>

                <ul>
                    <li class="fragment">Equivalent to a <b class="highlight-orange">batch job</b> at regular interval</li>
                    <li class="fragment">API identical in usage to Datasets APIs</li>
                    <li class="fragment">Guarantee the events processing and delivery <b class="highlight-orange">order</b></li>
                    <li class="fragment"><b class="highlight-orange">Fault tolerance</b> is handled entirely by API</li>
                    <li class="fragment">Capable to filter out or handle <b class="highlight-orange">out-of-order data</b></li>
                    <li class="fragment">Allow to join a stream against a static DataFrame</li>
                    <li class="fragment">Change queries at runtime</li>
                </ul>

            </section>

            <section>

                <h2>Identical API</h2>

                <p>Basic dataframe processing</p>

                <pre><code data-trim class="scala">
val inputDF = spark.read.json("s3://logs")

// Do operations using the standard DataFrame API and write to MySQL
inputDF.groupBy($"action", window($"time", "1 hour")).count()
       .write.format("jdbc")
       .save("jdbc:mysql//...")
                 </code></pre>

                <p>With structured streaming</p>

                <pre><code data-trim class="scala">
val inputDF = spark.readStream.json("s3://logs")

// Do operations using the standard DataFrame API and write to MySQL
inputDF.groupBy($"action", window($"time", "1 hour")).count()
       .writeStream.format("jdbc")
       .start("jdbc:mysql//...")
                 </code></pre>

            </section>

            <section>

                <h2>Joining Streams with static data</h2>

                <pre><code data-trim class="scala">
// Bring in data about each customers from a static table,
// then join it with a streaming Dataframe
val customersDF = spark.table("customers")
inputStreamDF.join(customersDF, "customer_id")
.groupBy($"customer_name", hour($"time"))
.count()
                </code></pre>

            </section>

            <section>

                <h2>Unbounded table</h2>

                <img style="background: white" class="logo" src="img/unbounded-table.png"/>

            </section>

            <section>

                <h2>Incrementalization</h2>

                <p>Converts the batch-like query to a streaming execution plan</p>

                <img style="background: white" class="logo" src="img/incrementalisation.png"/>

            </section>

            <section>

                <h2 style="margin-bottom: 1em">Ouput modes</h2>

                <p style="margin-bottom: 1.5em">There is 3 ways to write the results of a stream processing</p>

                <ul>
                    <li><b>Append</b> : write only the new rows to the result table</li>
                    <li><b>Complete</b> : write entire result table each time</li>
                    <li><b>Update</b> : write only updated rows in the result table</li>
                </ul>

            </section>

            <section>

                <h2>Fault tolerance</h2>

                <p>Spark impose two requirements on datasources to guarantee fault tolerance</p>

                <ul>
                    <li class="fragment"><b class="highlight-orange">Input sources</b> must be <b class="highlight-orange">replayable</b>, so that recent data can be re-read if the job crashes</li>
                    <li class="fragment"><b class="highlight-orange">Output sinks</b> must support <b class="highlight-orange">transactional updates</b>, so that the system can make a set of records appear atomically</li>
                </ul>

            </section>

            <section>

                <h2>Not yet supported</h2>

                <ul>
                    <li>Limit and take first N rows</li>
                    <li>Distinct operations</li>
                    <li>Sorting operations limited</li>
                    <li>Limited outer joins between streamed and static data</li>
                    <li>No support for joining streams</li>
                </ul>

            </section>

        </section>

        <section>

            <h2>Future</h2>

            <ul>
                <li>More sources/sinks for Structured Streaming</li>
                <li>Coverage of all SQL 2003 queries</li>
                <li>Improve Datasets APIs performances</li>
                <li>Focus on Spark.ML</li>
                <li>Add missing features to Python and R</li>
            </ul>

        </section>

        <section>

            <h2>Demo</h2>

            <a target="_blank" href="https://ssw2016.cloud.databricks.com/?o=4480125715694487#notebook/2720120816673847">Spark 2 Notebook</a>

        </section>

        <section>

            <h2>Any Question ?</h2>

        </section>

    </div>

</div>

<script src="../revealjs/lib/js/head.min.js"></script>
<script src="../revealjs/js/reveal.js"></script>

<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'convex', // none/fade/slide/convex/concave/zoom

        dependencies: [
            {
                src: '../revealjs/lib/js/classList.js', condition: function () {
                return !document.body.classList;
            }
            },
            {
                src: '../revealjs/plugin/markdown/marked.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: '../revealjs/plugin/markdown/markdown.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: '../revealjs/plugin/highlight/highlight.js', async: true, condition: function () {
                return !!document.querySelector('pre code');
            }, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            },
            {src: '../revealjs/plugin/zoom-js/zoom.js', async: true},
            {src: '../revealjs/plugin/notes/notes.js', async: true}
        ]
    });

</script>

</body>
</html>
