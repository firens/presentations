<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Spark 2</title>

    <meta name="description" content="Spark 2, loads of new features">
    <meta name="author" content="Samuel Durand">

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="../revealjs/css/reveal.css">
    <link rel="stylesheet" href="../revealjs/css/theme/black.css" id="theme">
    <link rel="stylesheet" href="css/spark2.css"/>

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="../revealjs/lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? '../revealjs/css/print/pdf.css' : '../revealjs/css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">

    <div class="slides">
        <section>
            <h1 style="text-transform: none">Spark 2.0</h1>
            <h3>Easier, Faster and Smarter ?</h3>
            <img class="logo" src="img/spark-logo.png"/>
            <p>
                <small>Presentation by <a href="https://twitter.com/samudurand">Samuel Durand</a></small>
            </p>
        </section>

        <section>
            <h3>Today we will talk about...</h3>
            <ul>
                <li>What is Spark ?</li>
                <li>Spark 2 : Goals</li>
                <li>New features and improvements</li>
                <li>Future</li>
                <li>Demo</li>
            </ul>
        </section>

        <section>

            <section>
                <h2 style="margin-bottom: 1em">Apache Spark</h2>
                <p>Created in <b class="highlight-blue">2009</b> by  Matei Zaharia</p>
                <p>Maintained since <b class="highlight-blue">2013</b> by the Apache Foundation</p>
                <br/>
                <p>Version <b class="highlight-orange">2.0</b> released in June <b class="highlight-blue">2016</b></li></p>
                <br/>
                <p>One of the most active Apache project</p>
            </section>

            <section>
                <h2 style="margin-bottom: 1em">What is Spark ?</h2>
                <p>Advertised as <i>"A fast and general engine for large-scale data processing"</i></p>
                <p>A <span class="highlight-orange">data processing</span> platform,
                    running on a <span class="highlight-orange">cluster</span></p>
                <p>Designed to handle very large amount of archived or live data very <span class="highlight-orange">quickly</span> and <span class="highlight-orange">efficiently</span>.</p>
            </section>

            <section>
                <h2>What is Spark ?</h2>
                <ul>
                    <li>Designed for <span class="highlight-orange">analytical</span>/transforming operations</li>
                    <li>Minimal amount code</li>
                    <li>Various languages : Scala, Java, Python... </li>
                    <li>Handle data <span class="highlight-orange">in memory</span></li>
                    <li>Very large cluster (> 8000 nodes)</li>
                </ul>
            </section>

            <section>
                <h2 style="margin-bottom: 1em">Features</h2>
                <ul>
                    <li>Data analysis</li>
                    <li>Work with RDDs or SQL like queries</li>
                    <li>Streaming</li>
                    <li>Machine Learning</li>
                    <li>APIs for many datasources <br/>(HDFS, Cassandra, Kafka...)</li>
                </ul>
            </section>

            <section>
                <h2>Spark Ecosystem</h2>
                <img class="logo" src="img/spark-ecosystem.png"/>
            </section>

            <section>
                <p>Code example ?</p>
                <pre><code data-trim  class="scala">
val rdd = sparkContext.textFile("hdfs://...")

rdd.flatMap(line => line.split(" "))
.map(word => (word, 1)).reduceByKey(_ + _)
.saveAsTextFile("hdfs://...")
                </code></pre>
            </section>

            <section>
                What you would do with Spark ? What you would not do ?
            </section>

            <section>
                <h2 style="margin-bottom: 1em">A note on Databricks</h2>
                <p>Founded by Matei Zaharia and other co-founders of Spark</p>
                <img class="logo" src="img/databricks-logo.png"/>
                <p>Provides services and support for Spark</p>
                <p>Also offers an <span class="highlight-orange">online platform</span> for experimenting/training (free option since June 2016)</p>
            </section>

        </section>

        <section>
            <h2>Spark 2 : What's new ?</h2>
            <ul>
                <li class="fragment">Focus on <i>Datasets</i> instead of <i>RDDs</i></li>
                <li class="fragment">Unify <i>Datasets</i> and <i>Dataframes</i></li>
                <li class="fragment"><b class="highlight-orange">SparkSession</b> : single entry point to data APIs</li>
                <li class="fragment"><b class="highlight-orange">SparkSQL</b> : all 99 TPC-DS queries</li>
                <li class="fragment">Performance improvements : <b class="highlight-orange">Tungsten</b></li>
                <li class="fragment">Spark.ML replaces MLlib</li>
                <li class="fragment">Window functions</li>
                <li class="fragment">Many more : CSV support, more features for R users...</li>
            </ul>
        </section>

        <section>

            <section>

                <h2>RDDs VS Datasets</h2>

                <p>Why choose to work with Datasets ?</p>

            </section>

            <section>

                <h2 style="margin-bottom: 1em">Resilient Distributed Dataset (RDD)</h2>

                <ul>
                    <li>Immutable distributed collection of Objects</li>
                    <li>Unstructured data</li>
                    <pre><code data-trim  class="scala">
val lines = sc.textFile("data.txt")
val total = lines.map(s => s.length)
                 .reduce((a, b) => a + b)
                    </code></pre>
                    <li>Functional programming</li>
                    <li>Syntax errors caught at compile time</li>
                    <li>Very limited optimizations possible</li>
                    <li>More control over operations</li>
                </ul>

            </section>

            <section>

                <h2 style="margin-bottom: 1em">Datasets</h2>

                <ul>
                    <li>Immutable distributed collection of Objects</li>
                    <li>Structured data, similar to tables</li>
                    <pre><code data-trim  class="scala">
val ds = spark.read.json("people.json").as[Person]
ds.map(teenager => "Name: " + teenager(0)).show()
                    </code></pre>
                    <li>3 APIs available : SQL, Dataframe, Dataset</li>
                    <li>Type safe</li>
                    <li>Syntax and analysis errors caught at compile time</li>
                    <li>Structure <b class="highlight-orange">limits</b> what can be done =>
                        <br/><b class="highlight-orange">Storage and performance optimizations</b></li>
                </ul>

            </section>

            <section>

                <h2>Optimizations</h2>

                <img src="img/memory-usage.png"/>

                <img src="img/performance.png"/>

            </section>

            <section>

                <h2>Structured APIs</h2>

                <p>SQL</p>
                    <pre><code data-trim  class="scala">
val peopleDF = spark.read.json("people.json")
peopleDF.createOrReplaceTempView("people")
spark.sql("SELECT name FROM people").show()
                    </code></pre>
                <p>Dataframes</p>
                    <pre><code data-trim  class="scala">
val peopleDF = spark.read.json("people.json")
peopleDF.select($"name", $"age" + 1).show()
                    </code></pre>
                <p>Datasets</p>
                <pre><code data-trim  class="scala">
val personDS = spark.read.json("people.json").as[Person]
peopleDs.foreach(p => println(s"| ${p.name} | ${p.age + 1} |"))
                    </code></pre>
            </section>
            
            <section>
                
                <h2>Structured APIs</h2>
                
                <img src="img/structured-apis.png">

                <div class="fragment">
                    <p >Analysis errors caught before job starts</p>

                    <pre><code data-trim  class="scala">
case class University(numStudents: Int)
val schools = sqlContext.read.json("/schools.json").as[University]
org.....AnalysisException:
  Cannot upcast `numStudents` from string to int as it may truncate
                    </code></pre>
                </div>
                
            </section>

        </section>



        <section>

            RDD : Object oriented design, cannot optimise as much as Dataframe

            Dataframe and SparkSQL : high level api to work with structured Data (database tables, JSON...)
                Let Spark automatically optimize storage and computation with Tungstene and Catalyst optimizer
                like working on data on raw binary form

            Dataset : extension of Dataframe with same advantages, but type-safe and object-oriented
            A Dataset is a strongly-typed, immutable collection of objects that are mapped to a relational schema.

            query planner

            Datasets also leverage Tungsten’s fast in-memory encoding

            compile-time type safety – meaning production applications can be checked for errors before they are run

            direct operations over user-defined classes

            At the core of the Dataset API is a new concept called an encoder, which is responsible for converting between JVM objects and tabular representation. The tabular representation is stored using Spark’s internal Tungsten binary format, allowing for operations on serialized data and improved memory utilization.

        </section>

    </div>

</div>

<script src="../revealjs/lib/js/head.min.js"></script>
<script src="../revealjs/js/reveal.js"></script>

<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'convex', // none/fade/slide/convex/concave/zoom

        dependencies: [
            {
                src: '../revealjs/lib/js/classList.js', condition: function () {
                return !document.body.classList;
            }
            },
            {
                src: '../revealjs/plugin/markdown/marked.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: '../revealjs/plugin/markdown/markdown.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: '../revealjs/plugin/highlight/highlight.js', async: true, condition: function () {
                return !!document.querySelector('pre code');
            }, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            },
            {src: '../revealjs/plugin/zoom-js/zoom.js', async: true},
            {src: '../revealjs/plugin/notes/notes.js', async: true}
        ]
    });

</script>

</body>
</html>
